{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MoA-NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO4icNPn1fup",
        "outputId": "0347ab92-2d1b-49ad-df83-d37a355e9a4a"
      },
      "source": [
        "!pip install 'scikit-learn==0.23.2'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.23.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 5.4MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2) (1.19.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2) (1.0.0)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.23.2 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJBMnkp0RMBj"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "pd.options.display.max_rows = 999\n",
        "pd.options.display.max_colwidth = 999"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF_XZGpohsLX",
        "outputId": "dfd8a2a9-29ce-4381-a2ef-53148492149f"
      },
      "source": [
        "# sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
        "\n",
        "!pip install iterative-stratification"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting iterative-stratification\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.19.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.23.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (1.0.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (2.1.0)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds_ufdWAyC5d"
      },
      "source": [
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ortvhScksgz2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules.loss import _WeightedLoss\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "torch.manual_seed(1)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJdp2b6fjy27"
      },
      "source": [
        "def get_logger(filename='log'):\n",
        "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return logger\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "seed_everything(seed=42)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5YdXFCnshRP",
        "outputId": "ddc537d3-1975-436f-b658-b269cd0ee74a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahUpYjYCsln5"
      },
      "source": [
        "# global variables\n",
        "\n",
        "BASE_PATH = './drive/My Drive/Kaggle-MoA/input_data/'\n",
        "NN_PATH = './drive/My Drive/Kaggle-MoA/NN_models/'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPUWZMdes2F4"
      },
      "source": [
        "train_features = pd.read_csv(BASE_PATH + 'train_features.csv')\n",
        "train_targets_scored = pd.read_csv(BASE_PATH + 'train_targets_scored.csv')\n",
        "train_targets_nonscored = pd.read_csv(BASE_PATH + 'train_targets_nonscored.csv')\n",
        "train_drug = pd.read_csv(BASE_PATH + 'train_drug.csv')\n",
        "test_features = pd.read_csv(BASE_PATH + 'test_features.csv')\n",
        "\n",
        "submission = pd.read_csv(BASE_PATH + 'sample_submission.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "BkI8RhAgBCTg",
        "outputId": "8854b6b7-9b08-4692-e092-f20a75667488"
      },
      "source": [
        "train_features.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>cp_type</th>\n",
              "      <th>cp_time</th>\n",
              "      <th>cp_dose</th>\n",
              "      <th>g-0</th>\n",
              "      <th>g-1</th>\n",
              "      <th>g-2</th>\n",
              "      <th>g-3</th>\n",
              "      <th>g-4</th>\n",
              "      <th>g-5</th>\n",
              "      <th>g-6</th>\n",
              "      <th>g-7</th>\n",
              "      <th>g-8</th>\n",
              "      <th>g-9</th>\n",
              "      <th>g-10</th>\n",
              "      <th>g-11</th>\n",
              "      <th>g-12</th>\n",
              "      <th>g-13</th>\n",
              "      <th>g-14</th>\n",
              "      <th>g-15</th>\n",
              "      <th>g-16</th>\n",
              "      <th>g-17</th>\n",
              "      <th>g-18</th>\n",
              "      <th>g-19</th>\n",
              "      <th>g-20</th>\n",
              "      <th>g-21</th>\n",
              "      <th>g-22</th>\n",
              "      <th>g-23</th>\n",
              "      <th>g-24</th>\n",
              "      <th>g-25</th>\n",
              "      <th>g-26</th>\n",
              "      <th>g-27</th>\n",
              "      <th>g-28</th>\n",
              "      <th>g-29</th>\n",
              "      <th>g-30</th>\n",
              "      <th>g-31</th>\n",
              "      <th>g-32</th>\n",
              "      <th>g-33</th>\n",
              "      <th>g-34</th>\n",
              "      <th>g-35</th>\n",
              "      <th>...</th>\n",
              "      <th>c-60</th>\n",
              "      <th>c-61</th>\n",
              "      <th>c-62</th>\n",
              "      <th>c-63</th>\n",
              "      <th>c-64</th>\n",
              "      <th>c-65</th>\n",
              "      <th>c-66</th>\n",
              "      <th>c-67</th>\n",
              "      <th>c-68</th>\n",
              "      <th>c-69</th>\n",
              "      <th>c-70</th>\n",
              "      <th>c-71</th>\n",
              "      <th>c-72</th>\n",
              "      <th>c-73</th>\n",
              "      <th>c-74</th>\n",
              "      <th>c-75</th>\n",
              "      <th>c-76</th>\n",
              "      <th>c-77</th>\n",
              "      <th>c-78</th>\n",
              "      <th>c-79</th>\n",
              "      <th>c-80</th>\n",
              "      <th>c-81</th>\n",
              "      <th>c-82</th>\n",
              "      <th>c-83</th>\n",
              "      <th>c-84</th>\n",
              "      <th>c-85</th>\n",
              "      <th>c-86</th>\n",
              "      <th>c-87</th>\n",
              "      <th>c-88</th>\n",
              "      <th>c-89</th>\n",
              "      <th>c-90</th>\n",
              "      <th>c-91</th>\n",
              "      <th>c-92</th>\n",
              "      <th>c-93</th>\n",
              "      <th>c-94</th>\n",
              "      <th>c-95</th>\n",
              "      <th>c-96</th>\n",
              "      <th>c-97</th>\n",
              "      <th>c-98</th>\n",
              "      <th>c-99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_000644bb2</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>24</td>\n",
              "      <td>D1</td>\n",
              "      <td>1.0620</td>\n",
              "      <td>0.5577</td>\n",
              "      <td>-0.2479</td>\n",
              "      <td>-0.6208</td>\n",
              "      <td>-0.1944</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>-1.0220</td>\n",
              "      <td>-0.0326</td>\n",
              "      <td>0.5548</td>\n",
              "      <td>-0.0921</td>\n",
              "      <td>1.1830</td>\n",
              "      <td>0.1530</td>\n",
              "      <td>0.5574</td>\n",
              "      <td>-0.4015</td>\n",
              "      <td>0.1789</td>\n",
              "      <td>-0.6528</td>\n",
              "      <td>-0.7969</td>\n",
              "      <td>0.6342</td>\n",
              "      <td>0.1778</td>\n",
              "      <td>-0.3694</td>\n",
              "      <td>-0.5688</td>\n",
              "      <td>-1.1360</td>\n",
              "      <td>-1.1880</td>\n",
              "      <td>0.6940</td>\n",
              "      <td>0.4393</td>\n",
              "      <td>0.2664</td>\n",
              "      <td>0.1907</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>-0.2853</td>\n",
              "      <td>0.5819</td>\n",
              "      <td>0.2934</td>\n",
              "      <td>-0.5584</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.3010</td>\n",
              "      <td>-0.1537</td>\n",
              "      <td>0.2198</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4805</td>\n",
              "      <td>0.4965</td>\n",
              "      <td>0.3680</td>\n",
              "      <td>0.8427</td>\n",
              "      <td>0.1042</td>\n",
              "      <td>0.1403</td>\n",
              "      <td>0.1758</td>\n",
              "      <td>1.2570</td>\n",
              "      <td>-0.5979</td>\n",
              "      <td>1.2250</td>\n",
              "      <td>-0.0553</td>\n",
              "      <td>0.7351</td>\n",
              "      <td>0.5810</td>\n",
              "      <td>0.9590</td>\n",
              "      <td>0.2427</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.4141</td>\n",
              "      <td>0.8432</td>\n",
              "      <td>0.6162</td>\n",
              "      <td>-0.7318</td>\n",
              "      <td>1.2120</td>\n",
              "      <td>0.6362</td>\n",
              "      <td>-0.4427</td>\n",
              "      <td>0.1288</td>\n",
              "      <td>1.4840</td>\n",
              "      <td>0.1799</td>\n",
              "      <td>0.5367</td>\n",
              "      <td>-0.1111</td>\n",
              "      <td>-1.0120</td>\n",
              "      <td>0.6685</td>\n",
              "      <td>0.2862</td>\n",
              "      <td>0.2584</td>\n",
              "      <td>0.8076</td>\n",
              "      <td>0.5523</td>\n",
              "      <td>-0.1912</td>\n",
              "      <td>0.6584</td>\n",
              "      <td>-0.3981</td>\n",
              "      <td>0.2139</td>\n",
              "      <td>0.3801</td>\n",
              "      <td>0.4176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_000779bfc</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.0743</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.2991</td>\n",
              "      <td>0.0604</td>\n",
              "      <td>1.0190</td>\n",
              "      <td>0.5207</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.3372</td>\n",
              "      <td>-0.4047</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>-1.1520</td>\n",
              "      <td>-0.4201</td>\n",
              "      <td>-0.0958</td>\n",
              "      <td>0.4590</td>\n",
              "      <td>0.0803</td>\n",
              "      <td>0.2250</td>\n",
              "      <td>0.5293</td>\n",
              "      <td>0.2839</td>\n",
              "      <td>-0.3494</td>\n",
              "      <td>0.2883</td>\n",
              "      <td>0.9449</td>\n",
              "      <td>-0.1646</td>\n",
              "      <td>-0.2657</td>\n",
              "      <td>-0.3372</td>\n",
              "      <td>0.3135</td>\n",
              "      <td>-0.4316</td>\n",
              "      <td>0.4773</td>\n",
              "      <td>0.2075</td>\n",
              "      <td>-0.4216</td>\n",
              "      <td>-0.1161</td>\n",
              "      <td>-0.0499</td>\n",
              "      <td>-0.2627</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>-0.2483</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>-0.2102</td>\n",
              "      <td>...</td>\n",
              "      <td>0.4083</td>\n",
              "      <td>0.0319</td>\n",
              "      <td>0.3905</td>\n",
              "      <td>0.7099</td>\n",
              "      <td>0.2912</td>\n",
              "      <td>0.4151</td>\n",
              "      <td>-0.2840</td>\n",
              "      <td>-0.3104</td>\n",
              "      <td>-0.6373</td>\n",
              "      <td>0.2887</td>\n",
              "      <td>-0.0765</td>\n",
              "      <td>0.2539</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>0.5932</td>\n",
              "      <td>0.2031</td>\n",
              "      <td>0.7639</td>\n",
              "      <td>0.5499</td>\n",
              "      <td>-0.3322</td>\n",
              "      <td>-0.0977</td>\n",
              "      <td>0.4329</td>\n",
              "      <td>-0.2782</td>\n",
              "      <td>0.7827</td>\n",
              "      <td>0.5934</td>\n",
              "      <td>0.3402</td>\n",
              "      <td>0.1499</td>\n",
              "      <td>0.4420</td>\n",
              "      <td>0.9366</td>\n",
              "      <td>0.8193</td>\n",
              "      <td>-0.4236</td>\n",
              "      <td>0.3192</td>\n",
              "      <td>-0.4265</td>\n",
              "      <td>0.7543</td>\n",
              "      <td>0.4708</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.2957</td>\n",
              "      <td>0.4899</td>\n",
              "      <td>0.1522</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>0.6077</td>\n",
              "      <td>0.7371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_000a6266a</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>48</td>\n",
              "      <td>D1</td>\n",
              "      <td>0.6280</td>\n",
              "      <td>0.5817</td>\n",
              "      <td>1.5540</td>\n",
              "      <td>-0.0764</td>\n",
              "      <td>-0.0323</td>\n",
              "      <td>1.2390</td>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>1.2300</td>\n",
              "      <td>-0.4797</td>\n",
              "      <td>-0.5631</td>\n",
              "      <td>-0.0366</td>\n",
              "      <td>-1.8300</td>\n",
              "      <td>0.6057</td>\n",
              "      <td>-0.3278</td>\n",
              "      <td>0.6042</td>\n",
              "      <td>-0.3075</td>\n",
              "      <td>-0.1147</td>\n",
              "      <td>-0.0570</td>\n",
              "      <td>-0.0799</td>\n",
              "      <td>-0.8181</td>\n",
              "      <td>-1.5320</td>\n",
              "      <td>0.2307</td>\n",
              "      <td>0.4901</td>\n",
              "      <td>0.4780</td>\n",
              "      <td>-1.3970</td>\n",
              "      <td>4.6240</td>\n",
              "      <td>-0.0437</td>\n",
              "      <td>1.2870</td>\n",
              "      <td>-1.8530</td>\n",
              "      <td>0.6069</td>\n",
              "      <td>0.4290</td>\n",
              "      <td>0.1783</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>-1.1800</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.5477</td>\n",
              "      <td>-0.7576</td>\n",
              "      <td>-0.0444</td>\n",
              "      <td>0.1894</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>-2.3640</td>\n",
              "      <td>-0.4682</td>\n",
              "      <td>0.1210</td>\n",
              "      <td>-0.5177</td>\n",
              "      <td>-0.0604</td>\n",
              "      <td>0.1682</td>\n",
              "      <td>-0.4436</td>\n",
              "      <td>0.4963</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.3335</td>\n",
              "      <td>0.9760</td>\n",
              "      <td>-0.0427</td>\n",
              "      <td>-0.1235</td>\n",
              "      <td>0.0959</td>\n",
              "      <td>0.0690</td>\n",
              "      <td>-0.9416</td>\n",
              "      <td>-0.7548</td>\n",
              "      <td>-0.1109</td>\n",
              "      <td>-0.6272</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>0.1172</td>\n",
              "      <td>0.1093</td>\n",
              "      <td>-0.3113</td>\n",
              "      <td>0.3019</td>\n",
              "      <td>-0.0873</td>\n",
              "      <td>-0.7250</td>\n",
              "      <td>-0.6297</td>\n",
              "      <td>0.6103</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>-1.3240</td>\n",
              "      <td>-0.3174</td>\n",
              "      <td>-0.6417</td>\n",
              "      <td>-0.2187</td>\n",
              "      <td>-1.4080</td>\n",
              "      <td>0.6931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_0015fd391</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>48</td>\n",
              "      <td>D1</td>\n",
              "      <td>-0.5138</td>\n",
              "      <td>-0.2491</td>\n",
              "      <td>-0.2656</td>\n",
              "      <td>0.5288</td>\n",
              "      <td>4.0620</td>\n",
              "      <td>-0.8095</td>\n",
              "      <td>-1.9590</td>\n",
              "      <td>0.1792</td>\n",
              "      <td>-0.1321</td>\n",
              "      <td>-1.0600</td>\n",
              "      <td>-0.8269</td>\n",
              "      <td>-0.3584</td>\n",
              "      <td>-0.8511</td>\n",
              "      <td>-0.5844</td>\n",
              "      <td>-2.5690</td>\n",
              "      <td>0.8183</td>\n",
              "      <td>-0.0532</td>\n",
              "      <td>-0.8554</td>\n",
              "      <td>0.1160</td>\n",
              "      <td>-2.3520</td>\n",
              "      <td>2.1200</td>\n",
              "      <td>-1.1580</td>\n",
              "      <td>-0.7191</td>\n",
              "      <td>-0.8004</td>\n",
              "      <td>-1.4670</td>\n",
              "      <td>-0.0107</td>\n",
              "      <td>-0.8995</td>\n",
              "      <td>0.2406</td>\n",
              "      <td>-0.2479</td>\n",
              "      <td>-1.0890</td>\n",
              "      <td>-0.7575</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>-2.7370</td>\n",
              "      <td>0.8745</td>\n",
              "      <td>0.5787</td>\n",
              "      <td>-1.6740</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.1220</td>\n",
              "      <td>-0.3752</td>\n",
              "      <td>-2.3820</td>\n",
              "      <td>-3.7350</td>\n",
              "      <td>-2.9740</td>\n",
              "      <td>-1.4930</td>\n",
              "      <td>-1.6600</td>\n",
              "      <td>-3.1660</td>\n",
              "      <td>0.2816</td>\n",
              "      <td>-0.2990</td>\n",
              "      <td>-1.1870</td>\n",
              "      <td>-0.5044</td>\n",
              "      <td>-1.7750</td>\n",
              "      <td>-1.6120</td>\n",
              "      <td>-0.9215</td>\n",
              "      <td>-1.0810</td>\n",
              "      <td>-3.0520</td>\n",
              "      <td>-3.4470</td>\n",
              "      <td>-2.7740</td>\n",
              "      <td>-1.8460</td>\n",
              "      <td>-0.5568</td>\n",
              "      <td>-3.3960</td>\n",
              "      <td>-2.9510</td>\n",
              "      <td>-1.1550</td>\n",
              "      <td>-3.2620</td>\n",
              "      <td>-1.5390</td>\n",
              "      <td>-2.4600</td>\n",
              "      <td>-0.9417</td>\n",
              "      <td>-1.5550</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>-2.0990</td>\n",
              "      <td>-0.6441</td>\n",
              "      <td>-5.6300</td>\n",
              "      <td>-1.3780</td>\n",
              "      <td>-0.8632</td>\n",
              "      <td>-1.2880</td>\n",
              "      <td>-1.6210</td>\n",
              "      <td>-0.8784</td>\n",
              "      <td>-0.3876</td>\n",
              "      <td>-0.8154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_001626bd3</td>\n",
              "      <td>trt_cp</td>\n",
              "      <td>72</td>\n",
              "      <td>D2</td>\n",
              "      <td>-0.3254</td>\n",
              "      <td>-0.4009</td>\n",
              "      <td>0.9700</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>1.4180</td>\n",
              "      <td>-0.8244</td>\n",
              "      <td>-0.2800</td>\n",
              "      <td>-0.1498</td>\n",
              "      <td>-0.8789</td>\n",
              "      <td>0.8630</td>\n",
              "      <td>-0.2219</td>\n",
              "      <td>-0.5121</td>\n",
              "      <td>-0.9577</td>\n",
              "      <td>1.1750</td>\n",
              "      <td>0.2042</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1244</td>\n",
              "      <td>-1.7090</td>\n",
              "      <td>-0.3543</td>\n",
              "      <td>-0.5160</td>\n",
              "      <td>-0.3330</td>\n",
              "      <td>-0.2685</td>\n",
              "      <td>0.7649</td>\n",
              "      <td>0.2057</td>\n",
              "      <td>1.3720</td>\n",
              "      <td>0.6835</td>\n",
              "      <td>0.8056</td>\n",
              "      <td>-0.3754</td>\n",
              "      <td>-1.2090</td>\n",
              "      <td>0.2965</td>\n",
              "      <td>-0.0712</td>\n",
              "      <td>0.6389</td>\n",
              "      <td>0.6674</td>\n",
              "      <td>-0.0783</td>\n",
              "      <td>1.1740</td>\n",
              "      <td>-0.7110</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.2274</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1535</td>\n",
              "      <td>-0.4640</td>\n",
              "      <td>-0.5943</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.1500</td>\n",
              "      <td>0.5178</td>\n",
              "      <td>0.5159</td>\n",
              "      <td>0.6091</td>\n",
              "      <td>0.1813</td>\n",
              "      <td>-0.4249</td>\n",
              "      <td>0.7832</td>\n",
              "      <td>0.6529</td>\n",
              "      <td>0.5648</td>\n",
              "      <td>0.4817</td>\n",
              "      <td>0.0587</td>\n",
              "      <td>0.5303</td>\n",
              "      <td>0.6376</td>\n",
              "      <td>-0.3966</td>\n",
              "      <td>-1.4950</td>\n",
              "      <td>-0.9625</td>\n",
              "      <td>-0.0541</td>\n",
              "      <td>0.6273</td>\n",
              "      <td>0.4563</td>\n",
              "      <td>0.0698</td>\n",
              "      <td>0.8134</td>\n",
              "      <td>0.1924</td>\n",
              "      <td>0.6054</td>\n",
              "      <td>-0.1824</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.6670</td>\n",
              "      <td>1.0690</td>\n",
              "      <td>0.5523</td>\n",
              "      <td>-0.3031</td>\n",
              "      <td>0.1094</td>\n",
              "      <td>0.2885</td>\n",
              "      <td>-0.3786</td>\n",
              "      <td>0.7125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 876 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         sig_id cp_type  cp_time cp_dose  ...    c-96    c-97    c-98    c-99\n",
              "0  id_000644bb2  trt_cp       24      D1  ... -0.3981  0.2139  0.3801  0.4176\n",
              "1  id_000779bfc  trt_cp       72      D1  ...  0.1522  0.1241  0.6077  0.7371\n",
              "2  id_000a6266a  trt_cp       48      D1  ... -0.6417 -0.2187 -1.4080  0.6931\n",
              "3  id_0015fd391  trt_cp       48      D1  ... -1.6210 -0.8784 -0.3876 -0.8154\n",
              "4  id_001626bd3  trt_cp       72      D2  ...  0.1094  0.2885 -0.3786  0.7125\n",
              "\n",
              "[5 rows x 876 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-MBnKpv_K_K"
      },
      "source": [
        "## Data Preprocessing: QuantileTransformer + PCA + Variance Thresholding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgayIZAmhORh"
      },
      "source": [
        "from sklearn.preprocessing import QuantileTransformer, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import pickle"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwsytA4koJ3o"
      },
      "source": [
        "def preprocessing_nn_train_only(train_features_raw, test_features_raw, random_state=42, is_train=True, n_quantiles=100, pca_g=600, pca_c=50, variance_threshold=0.8):\n",
        "\n",
        "    train_features = train_features_raw.copy()\n",
        "    test_features = test_features_raw.copy()\n",
        "\n",
        "    g_cols = [col for col in train_features.columns if col.startswith('g-')]\n",
        "    c_cols = [col for col in train_features.columns if col.startswith('c-')]\n",
        "    other_cols = [col for col in train_features.columns if col not in g_cols and col not in c_cols]\n",
        "\n",
        "    # QuantileTransformer\n",
        "    transformer = QuantileTransformer(n_quantiles=n_quantiles, random_state=random_state, output_distribution=\"normal\")\n",
        "    if is_train:\n",
        "        transformer.fit(train_features[g_cols + c_cols].values.reshape(-1, len(g_cols)+len(c_cols)))\n",
        "        pickle.dump(transformer, open(f\"{NN_PATH}SEED{random_state}_transformer.pkl\", 'wb'))\n",
        "    else:\n",
        "        transformer = pickle.load(open(f\"{NN_PATH}SEED{random_state}_transformer.pkl\", 'rb'))\n",
        "    \n",
        "    train_features[g_cols + c_cols] = transformer.transform(train_features[g_cols + c_cols].values.reshape(-1, len(g_cols)+len(c_cols)))\n",
        "    test_features[g_cols + c_cols] = transformer.transform(test_features[g_cols + c_cols].values.reshape(-1, len(g_cols)+len(c_cols)))\n",
        "\n",
        "    # PCA\n",
        "    if is_train:\n",
        "        pca_g = PCA(n_components=pca_g, random_state=random_state)\n",
        "        pca_g.fit(train_features[g_cols])\n",
        "        pickle.dump(pca_g, open(f\"{NN_PATH}SEED{random_state}_pca_g.pkl\", 'wb'))\n",
        "        \n",
        "        pca_c = PCA(n_components=pca_c, random_state=random_state)\n",
        "        train_c = pca_c.fit(train_features[c_cols])\n",
        "        pickle.dump(pca_c, open(f\"{NN_PATH}SEED{random_state}_pca_c.pkl\", 'wb'))\n",
        "    else:\n",
        "        pca_g = pickle.load(open(f\"{NN_PATH}SEED{random_state}_pca_g.pkl\", 'rb'))\n",
        "        pca_c = pickle.load(open(f\"{NN_PATH}SEED{random_state}_pca_c.pkl\", 'rb'))\n",
        "        \n",
        "    train_g = pca_g.transform(train_features[g_cols])\n",
        "    test_g = pca_g.transform(test_features[g_cols])\n",
        "    train_c = pca_c.transform(train_features[c_cols])\n",
        "    test_c = pca_c.transform(test_features[c_cols])\n",
        "    \n",
        "    train_g = pd.DataFrame(train_g, columns=['g_pca_{}'.format(i) for i in range(train_g.shape[1])])\n",
        "    test_g = pd.DataFrame(test_g, columns=['g_pca_{}'.format(i) for i in range(test_g.shape[1])])\n",
        "    train_c = pd.DataFrame(train_c, columns=['c_pca_{}'.format(i) for i in range(train_c.shape[1])])\n",
        "    test_c = pd.DataFrame(test_c, columns=['c_pca_{}'.format(i) for i in range(test_c.shape[1])])\n",
        "\n",
        "    train_features_pca = pd.concat([train_features[g_cols + c_cols], train_g, train_c], axis=1)\n",
        "    test_features_pca = pd.concat([test_features[g_cols + c_cols], test_g, test_c], axis=1)\n",
        "\n",
        "    # Varaince thresholding\n",
        "    if is_train:\n",
        "        variance_threshold = VarianceThreshold(variance_threshold)\n",
        "        variance_threshold.fit(train_features_pca)\n",
        "        pickle.dump(variance_threshold, open(f\"{NN_PATH}SEED{random_state}_variance.pkl\", 'wb'))\n",
        "    else:\n",
        "        variance_threshold = pickle.load(open(f\"{NN_PATH}SEED{random_state}_variance.pkl\", 'rb'))\n",
        "    \n",
        "    train_features_variance = variance_threshold.transform(train_features_pca)\n",
        "    test_features_variance = variance_threshold.transform(test_features_pca)\n",
        "\n",
        "    train_features_variance = pd.DataFrame(train_features_variance, columns=['col_{}'.format(i) for i in range(train_features_variance.shape[1])])\n",
        "    test_features_variance = pd.DataFrame(test_features_variance,  columns=['col_{}'.format(i) for i in range(test_features_variance.shape[1])])\n",
        "    \n",
        "    # categorical variable encoding\n",
        "    train_features_processed = pd.concat([train_features[other_cols], train_features_variance], axis=1)\n",
        "    test_features_processed = pd.concat([test_features[other_cols], test_features_variance], axis=1)\n",
        "\n",
        "    for col in ['cp_time', 'cp_dose']:\n",
        "        le = LabelEncoder()\n",
        "        train_features_processed[col] = le.fit_transform(train_features_processed[col])\n",
        "        test_features_processed[col] = le.transform(test_features_processed[col])\n",
        "\n",
        "    return train_features_processed, test_features_processed"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS1y5tzWyHzU"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qM25TZoQA7n"
      },
      "source": [
        "class MoADataset:\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "        \n",
        "    def __len__(self):\n",
        "        return (self.features.shape[0])\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        dct = {\n",
        "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
        "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n",
        "        }\n",
        "        \n",
        "        return dct\n",
        "\n",
        "class TestDataset:\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "        \n",
        "    def __len__(self):\n",
        "        return (self.features.shape[0])\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        dct = {\n",
        "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "        return dct"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8XjzQ1HfA3I"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_features, hidden_sizes, dropout_values):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.num_features = num_features\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.dropout_values = dropout_values\n",
        "        self.frozen_layers = []\n",
        "\n",
        "        self.mlp = nn.Sequential()\n",
        "        num_dim = num_features\n",
        "        for i, (hidden_size, dropout) in enumerate(zip(hidden_sizes, dropout_values)):\n",
        "            self.mlp.add_module(f'batch_norm_{i}', nn.BatchNorm1d(num_dim))\n",
        "            self.mlp.add_module(f'dropout_{i}', nn.Dropout(dropout))\n",
        "\n",
        "            if i != len(hidden_sizes)-1:\n",
        "                self.mlp.add_module(f'linear_{i}', nn.Linear(num_dim, hidden_size))\n",
        "                self.mlp.add_module(f'activation_{i}', nn.LeakyReLU(0.01))\n",
        "            else:\n",
        "                self.mlp.add_module(f'linear_{i}', nn.utils.weight_norm(nn.Linear(num_dim, hidden_size)))\n",
        "\n",
        "            # update the current dimension\n",
        "            num_dim = hidden_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "    def freeze(self):\n",
        "        \"\"\" freeze all the layers \"\"\"\n",
        "\n",
        "        for name, param in self.mlp.named_parameters():\n",
        "            layer_index = name.split('.')[0].split('_')[-1]\n",
        "            \n",
        "            # freeze the parameters\n",
        "            param.requires_grad = False\n",
        "        \n",
        "            # Save frozen layer names\n",
        "            if layer_index not in self.frozen_layers:\n",
        "                self.frozen_layers.append(layer_index)\n",
        "\n",
        "    def unfreeze(self):\n",
        "        \"\"\" un-freeze the last layer that is frozen \"\"\"\n",
        "\n",
        "        layer_index_to_defreeze = self.frozen_layers.pop()\n",
        "\n",
        "        for name, param in self.mlp.named_parameters():\n",
        "            layer_index = name.split('.')[0].split('_')[-1]\n",
        "\n",
        "            if layer_index == layer_index_to_defreeze:\n",
        "                param.requires_grad = True"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_UC3fLKQxjl"
      },
      "source": [
        "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    \n",
        "    for data in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        final_loss += loss.item()\n",
        "        \n",
        "    final_loss /= len(dataloader)\n",
        "    return final_loss\n",
        "\n",
        "def valid_fn(model, loss_fn, dataloader, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    valid_preds = []\n",
        "    \n",
        "    for data in dataloader:\n",
        "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        final_loss += loss.item()\n",
        "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
        "        \n",
        "    final_loss /= len(dataloader)\n",
        "    valid_preds = np.concatenate(valid_preds)\n",
        "    return final_loss, valid_preds\n",
        "\n",
        "def inference_fn(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    \n",
        "    for data in dataloader:\n",
        "        inputs = data['x'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "        \n",
        "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
        "        \n",
        "    preds = np.concatenate(preds)\n",
        "    return preds"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NavYXtY1Qztw"
      },
      "source": [
        "class SmoothBCEwLogits(_WeightedLoss):\n",
        "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
        "        super().__init__(weight=weight, reduction=reduction)\n",
        "        self.smoothing = smoothing\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "\n",
        "    @staticmethod\n",
        "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
        "        assert 0 <= smoothing < 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
        "            \n",
        "        return targets\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
        "            self.smoothing)\n",
        "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
        "\n",
        "        if  self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif  self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOeMQUm2mghC"
      },
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "            \n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))   "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IDgXoS-Rxwj"
      },
      "source": [
        "# HyperParameters\n",
        "\n",
        "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "EPOCHS = 24\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "WEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\n",
        "MAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\n",
        "DIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\n",
        "\n",
        "PCT_START = 0.1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdgqZJmXnqcw"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dxvxkweMW3j"
      },
      "source": [
        "class NNTrainer:\n",
        "    def __init__(self, params, train_features, test_features, train_targets_scored, train_targets_nonscored, train_drug, sample_submission):\n",
        "        super(NNTrainer, self).__init__()\n",
        "        \n",
        "        self.params = params\n",
        "\n",
        "        self.train_features = train_features\n",
        "        self.test_features = test_features\n",
        "        self.train_targets_scored = train_targets_scored\n",
        "        self.train_targets_nonscored = train_targets_nonscored\n",
        "        self.train_drug = train_drug\n",
        "        self.sample_submission = sample_submission\n",
        "\n",
        "        self.target_cols = [col for col in train_targets_scored.columns if col != 'sig_id']\n",
        "        self.target_nonscored_cols = [col for col in train_targets_nonscored.columns if col != 'sig_id']\n",
        "\n",
        "    def _set_seed(self, seed_id):\n",
        "        self.seed_id = seed_id\n",
        "        seed_everything(self.seed_id)\n",
        "\n",
        "    def _preprocess(self, seed_id, is_train=True):\n",
        "\n",
        "        train_features, test_features, train_targets_scored = self.train_features, self.test_features, self.train_targets_scored\n",
        "        train_drug = self.train_drug\n",
        "\n",
        "        train_features_processed, test_features_processed = preprocessing_nn_train_only(\n",
        "            train_features,\n",
        "            test_features,\n",
        "            random_state=seed_id,\n",
        "            is_train=is_train,\n",
        "            pca_g=self.params['pca_g'],\n",
        "            pca_c=self.params['pca_c'],\n",
        "            variance_threshold=self.params['variance_threshold']\n",
        "        )\n",
        "\n",
        "        train = train_features_processed \\\n",
        "            .merge(train_targets_scored, on='sig_id') \\\n",
        "            .merge(train_targets_nonscored, on='sig_id') \\\n",
        "            .merge(train_drug, on='sig_id')\n",
        "        \n",
        "        train = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True).drop('cp_type', axis=1)\n",
        "        test = test_features_processed[test_features_processed['cp_type'] != 'ctl_vehicle'].reset_index(drop=True).drop('cp_type', axis=1)\n",
        "        feature_cols = [col for col in train \\\n",
        "                                if col not in self.target_cols \\\n",
        "                                and col not in self.target_nonscored_cols \\\n",
        "                                and col != 'sig_id' \\\n",
        "                                and col != 'drug_id']\n",
        "\n",
        "        return train, test, feature_cols\n",
        "\n",
        "    def _split_kfold(self, train, num_folds=5, drug_threshold=18):\n",
        "\n",
        "        vc = train['drug_id'].value_counts()\n",
        "        vc1 = vc.loc[vc <= drug_threshold].index.sort_values()\n",
        "        vc2 = vc.loc[vc > drug_threshold].index.sort_values()\n",
        "\n",
        "        # STRATIFY DRUGS 18X OR LESS\n",
        "        dict1 = {}\n",
        "        dict2 = {}\n",
        "\n",
        "        kfold = MultilabelStratifiedKFold(n_splits=num_folds, shuffle=True, random_state=self.seed_id)\n",
        "        tmp = train.groupby('drug_id')[self.target_cols].mean().loc[vc1]\n",
        "\n",
        "        for fold, (idxT, idxV) in enumerate(kfold.split(tmp, tmp[self.target_cols])):\n",
        "            dd = {k: fold for k in tmp.index[idxV].values}\n",
        "            dict1.update(dd)\n",
        "\n",
        "        # STRATIFY DRUGS MORE THAN 18X\n",
        "        kfold = MultilabelStratifiedKFold(n_splits=num_folds, shuffle=True, random_state=self.seed_id)\n",
        "        tmp = train.loc[train['drug_id'].isin(vc2)].reset_index(drop=True)\n",
        "\n",
        "        for fold, (idxT, idxV) in enumerate(kfold.split(tmp, tmp[self.target_cols])):\n",
        "            dd = {k: fold for k in tmp.sig_id[idxV].values}\n",
        "            dict2.update(dd)\n",
        "\n",
        "        # ASSIGN FOLDS\n",
        "        train['fold'] = train['drug_id'].map(dict1)\n",
        "        train.loc[train['fold'].isna(), 'fold'] = train.loc[train['fold'].isna(), 'sig_id'].map(dict2)\n",
        "        train['fold'] = train['fold'].astype('int8')\n",
        "\n",
        "        return train\n",
        "\n",
        "    def _train_single_fold(self, model, seed_id, fold_id, targets='ALL_TARGETS'):\n",
        "\n",
        "        feature_cols, target_cols = self.feature_cols.copy(), self.target_cols.copy()\n",
        "        if targets == 'ALL_TARGETS':\n",
        "            target_cols += self.target_nonscored_cols\n",
        "\n",
        "        df_train = self.train[self.train['fold'] != fold_id]\n",
        "        df_val = self.train[self.train['fold'] == fold_id]\n",
        "\n",
        "        train_idx, val_idx = df_train.index, df_val.index\n",
        "\n",
        "        X_train, y_train = df_train[feature_cols], df_train[target_cols]\n",
        "        X_val, y_val = df_val[feature_cols], df_val[target_cols]\n",
        "\n",
        "        train_dataset = MoADataset(X_train.values, y_train.values)\n",
        "        val_dataset = MoADataset(X_val.values, y_val.values)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        optimizer = Adam(model.parameters(), lr=self.params['lr'], weight_decay=WEIGHT_DECAY[targets])\n",
        "        scheduler = OneCycleLR(optimizer=optimizer,\n",
        "                               steps_per_epoch=len(train_loader),\n",
        "                               pct_start=PCT_START,\n",
        "                               div_factor=DIV_FACTOR[targets],\n",
        "                               max_lr=MAX_LR[targets],\n",
        "                               epochs=EPOCHS\n",
        "                               )\n",
        "        \n",
        "        loss_fn = nn.BCEWithLogitsLoss()\n",
        "        loss_tr = SmoothBCEwLogits(smoothing=0.001)\n",
        "\n",
        "        oof = np.zeros((len(self.train), len(target_cols)))\n",
        "        best_loss = np.inf\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "\n",
        "            # gradually de-freeze layers if in the fine-tuning mode\n",
        "            if targets == 'SCORED_ONLY' and len(model.frozen_layers) > 0 and epoch % 4 == 0:\n",
        "                model.unfreeze()\n",
        "\n",
        "            train_loss = train_fn(model, optimizer, scheduler, loss_tr, train_loader, DEVICE)\n",
        "            val_loss, val_preds = valid_fn(model, loss_fn, val_loader, DEVICE)\n",
        "\n",
        "            if np.isnan(val_loss):\n",
        "                break\n",
        "\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                oof[val_idx] = val_preds\n",
        "                torch.save(model.state_dict(), f\"SEED{seed_id}_FOLD{fold_id}.pth\")\n",
        "                torch.save(model.state_dict(), f\"{NN_PATH}SEED{seed_id}_FOLD{fold_id}.pth\")\n",
        "\n",
        "        print(f\"SEED: {self.seed_id}, FOLD: {fold_id}, targets: {targets}, best train_loss: {train_loss:.6f}, best val_loss: {val_loss:.6f}\")\n",
        "\n",
        "        return oof\n",
        "\n",
        "    def _evaluate_single_model(self, seed_id, num_folds):\n",
        "\n",
        "        num_features = len(self.feature_cols)\n",
        "        num_targets = len(self.target_cols)\n",
        "        loss_fn = nn.BCEWithLogitsLoss()\n",
        "        oof = np.zeros((len(self.train), num_targets))\n",
        "\n",
        "        for fold_id in range(num_folds):\n",
        "            # load the best model\n",
        "            model = Model(num_features,\n",
        "                          self.params['hidden_sizes'] + [num_targets],\n",
        "                          self.params['dropout_values'])\n",
        "            model.load_state_dict(torch.load(f\"{NN_PATH}SEED{seed_id}_FOLD{fold_id}.pth\"))\n",
        "            model = model.to(DEVICE)\n",
        "\n",
        "            df_val = self.train[self.train['fold'] == fold_id]\n",
        "            val_idx = df_val.index\n",
        "\n",
        "            X_val, y_val = df_val[self.feature_cols], df_val[self.target_cols]\n",
        "            val_dataset = MoADataset(X_val.values, y_val.values)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "            val_loss, val_preds = valid_fn(model, loss_fn, val_loader, DEVICE)\n",
        "            oof[val_idx] = val_preds\n",
        "\n",
        "        return oof\n",
        "\n",
        "    def _predict_single_fold(self, seed_id, fold_id):\n",
        "\n",
        "        num_features = len(self.feature_cols)\n",
        "        num_targets = len(self.target_cols)\n",
        "\n",
        "        # load the best model\n",
        "        model = Model(num_features,\n",
        "                      self.params['hidden_sizes'] + [num_targets],\n",
        "                      self.params['dropout_values'])\n",
        "        model.load_state_dict(torch.load(f\"{NN_PATH}SEED{seed_id}_FOLD{fold_id}.pth\"))\n",
        "        model = model.to(DEVICE)\n",
        "\n",
        "        # prediction\n",
        "        X_test = self.test[self.feature_cols]\n",
        "        test_dataset = TestDataset(X_test.values)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        predictions = np.zeros((len(X_test), num_targets))\n",
        "        predictions = inference_fn(model, test_loader, DEVICE)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def _transfer_model(self, model_old):\n",
        "\n",
        "        num_targets = len(self.target_cols)\n",
        "\n",
        "        # create a new model from scratch and transfer the parameters over\n",
        "        model_new = Model(model_old.num_features,\n",
        "                          model_old.hidden_sizes,\n",
        "                          model_old.dropout_values\n",
        "                          ).to(DEVICE)\n",
        "        model_new.load_state_dict(model_old.state_dict())\n",
        "\n",
        "        # do not transfer the last one layer\n",
        "        # add the last layer with updated num_targets\n",
        "        last_index = len(model_old.hidden_sizes)-1\n",
        "        del model_new.mlp[-1]\n",
        "        model_new.mlp.add_module(f'linear_{last_index}', nn.utils.weight_norm(nn.Linear(model_new.hidden_sizes[-2], num_targets)))\n",
        "        model_new.hidden_sizes[-1] = num_targets\n",
        "\n",
        "        model_new = model_new.to(DEVICE)\n",
        "\n",
        "        # freeze all the layers\n",
        "        model_new.freeze()\n",
        "\n",
        "        return model_new\n",
        "\n",
        "    def _get_cv_score(self, train, train_targets_scored):\n",
        "\n",
        "        target_cols = self.target_cols\n",
        "\n",
        "        val_results = train_targets_scored.drop(columns=target_cols) \\\n",
        "            .merge(train[['sig_id']+target_cols], on='sig_id', how='left') \\\n",
        "            .fillna(0)\n",
        "\n",
        "        y_true = train_targets_scored[target_cols].values\n",
        "        y_pred = val_results[target_cols].values\n",
        "\n",
        "        score = 0\n",
        "        for i in range(len(target_cols)):\n",
        "            score += log_loss(y_true[:, i], y_pred[:, i])\n",
        "\n",
        "        return score / y_pred.shape[1]\n",
        "\n",
        "    def run_multiple_seeds(self, seeds, num_folds, run_type=\"training\"):\n",
        "\n",
        "        train_oof = self.train_features[self.train_features['cp_type'] != 'ctl_vehicle'][['sig_id']].copy()\n",
        "        train_oof[self.target_cols] = 0.0\n",
        "\n",
        "        test_preds = self.test_features[self.test_features['cp_type'] != 'ctl_vehicle'][['sig_id']].copy()\n",
        "        test_preds[self.target_cols] = 0.0\n",
        "\n",
        "        for seed_id in seeds:\n",
        "\n",
        "            # preprocess\n",
        "            self._set_seed(seed_id)\n",
        "            self.train, self.test, self.feature_cols = self._preprocess(seed_id=seed_id, is_train=(run_type==\"training\"))\n",
        "            num_features = len(self.feature_cols)\n",
        "            num_targets = len(self.target_cols)\n",
        "            num_targets_nonscored = len(self.target_nonscored_cols)\n",
        "\n",
        "            if run_type == \"training\":\n",
        "\n",
        "                # CV split\n",
        "                self.train = self._split_kfold(self.train, num_folds)\n",
        "\n",
        "                # kfold training\n",
        "                for fold_id in range(num_folds):\n",
        "\n",
        "                    # pretrain the model on all the targets (scored + nonscored)\n",
        "                    model = Model(num_features,\n",
        "                                  self.params['hidden_sizes'] + [num_targets+num_targets_nonscored],\n",
        "                                  self.params['dropout_values'])\n",
        "                    model = model.to(DEVICE)\n",
        "                    _ = self._train_single_fold(model, seed_id, fold_id, targets='ALL_TARGETS')\n",
        "            \n",
        "                    # load the best pretrained model\n",
        "                    pretrained_model = Model(num_features,\n",
        "                                             self.params['hidden_sizes'] + [num_targets+num_targets_nonscored],\n",
        "                                             self.params['dropout_values'])\n",
        "                    pretrained_model.load_state_dict(torch.load(f\"SEED{seed_id}_FOLD{fold_id}.pth\"))\n",
        "                    pretrained_model = pretrained_model.to(DEVICE)\n",
        "\n",
        "                    # transfer the model\n",
        "                    final_model = self._transfer_model(pretrained_model)\n",
        "\n",
        "                    # Fine-tune the model on scored targets only\n",
        "                    oof = self._train_single_fold(final_model, seed_id, fold_id, targets='SCORED_ONLY')\n",
        "\n",
        "                    # Accumulate OOF\n",
        "                    train_oof[self.target_cols] += oof / len(seeds)\n",
        "            elif run_type == \"evaluation\":\n",
        "\n",
        "                # CV split\n",
        "                self.train = self._split_kfold(self.train, num_folds)\n",
        "\n",
        "                # oof preds\n",
        "                oof = self._evaluate_single_model(seed_id, num_folds)\n",
        "                train_oof[self.target_cols] += oof / len(seeds)\n",
        "            else:\n",
        "                ### model inference\n",
        "                for fold_id in range(num_folds):\n",
        "                    predictions = self._predict_single_fold(seed_id, fold_id)\n",
        "                    test_preds[self.target_cols] += predictions / (len(seeds)*num_folds)\n",
        "\n",
        "        if run_type == \"training\":\n",
        "            cv_score = self._get_cv_score(train_oof, self.train_targets_scored)\n",
        "            return cv_score\n",
        "        elif run_type == \"evaluation\":\n",
        "            cv_score = self._get_cv_score(train_oof, self.train_targets_scored)\n",
        "            return cv_score, train_oof\n",
        "        else:\n",
        "            submission = self._create_submission(self.sample_submission, test_preds)\n",
        "            return submission\n",
        "\n",
        "    def _create_submission(self, sample_submission, test_preds):\n",
        "\n",
        "        submission = sample_submission.drop(columns=self.target_cols) \\\n",
        "            .merge(test_preds, on='sig_id', how='left') \\\n",
        "            .fillna(0)\n",
        "\n",
        "        return submission"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUj32EZokz6C"
      },
      "source": [
        "params = {\n",
        "    'pca_g': 600,\n",
        "    'pca_c': 80,\n",
        "    'variance_threshold': 0.8,\n",
        "    'hidden_sizes': [2000, 1500, 1000, 500],\n",
        "    'dropout_values': [0, 0.5, 0.4, 0.3, 0.1],\n",
        "    'lr': 1e-3\n",
        "}"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU-GmTRPRx56"
      },
      "source": [
        "nn_trainer = NNTrainer(params, train_features, test_features, train_targets_scored, train_targets_nonscored, train_drug, submission)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxHGC_KVQrc7",
        "outputId": "e56070bd-b27c-485a-c65e-83926c02bfe7"
      },
      "source": [
        "%%time\n",
        "\n",
        "cv_score = nn_trainer.run_multiple_seeds(seeds=[42], num_folds=7, run_type=\"training\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=42 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=42 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SEED: 42, FOLD: 0, targets: ALL_TARGETS, best train_loss: 0.012180, best val_loss: 0.009017\n",
            "SEED: 42, FOLD: 0, targets: SCORED_ONLY, best train_loss: 0.018225, best val_loss: 0.017099\n",
            "SEED: 42, FOLD: 1, targets: ALL_TARGETS, best train_loss: 0.012123, best val_loss: 0.009221\n",
            "SEED: 42, FOLD: 1, targets: SCORED_ONLY, best train_loss: 0.018174, best val_loss: 0.017034\n",
            "SEED: 42, FOLD: 2, targets: ALL_TARGETS, best train_loss: 0.012157, best val_loss: 0.009150\n",
            "SEED: 42, FOLD: 2, targets: SCORED_ONLY, best train_loss: 0.018046, best val_loss: 0.017672\n",
            "SEED: 42, FOLD: 3, targets: ALL_TARGETS, best train_loss: 0.012230, best val_loss: 0.008963\n",
            "SEED: 42, FOLD: 3, targets: SCORED_ONLY, best train_loss: 0.018522, best val_loss: 0.016700\n",
            "SEED: 42, FOLD: 4, targets: ALL_TARGETS, best train_loss: 0.012125, best val_loss: 0.009292\n",
            "SEED: 42, FOLD: 4, targets: SCORED_ONLY, best train_loss: 0.018032, best val_loss: 0.017419\n",
            "SEED: 42, FOLD: 5, targets: ALL_TARGETS, best train_loss: 0.012115, best val_loss: 0.009495\n",
            "SEED: 42, FOLD: 5, targets: SCORED_ONLY, best train_loss: 0.018302, best val_loss: 0.017706\n",
            "SEED: 42, FOLD: 6, targets: ALL_TARGETS, best train_loss: 0.012148, best val_loss: 0.009191\n",
            "SEED: 42, FOLD: 6, targets: SCORED_ONLY, best train_loss: 0.018311, best val_loss: 0.016583\n",
            "CPU times: user 10min 14s, sys: 20 s, total: 10min 33s\n",
            "Wall time: 10min 54s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYY1mTzQQrgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57cf4441-f1e2-437c-d77d-a2f39bddcde4"
      },
      "source": [
        "print(cv_score)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.015862005413236926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RohT3tLL-Dq-",
        "outputId": "67644430-f947-421b-c2b8-2cbbdae75a4b"
      },
      "source": [
        "cv_score, train_oof = nn_trainer.run_multiple_seeds(seeds=[42], num_folds=7, run_type=\"evaluation\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=42 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=42 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNOUPxoK-Du1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7fb97d1-9b7d-4c75-94b0-43ee049be953"
      },
      "source": [
        "print(cv_score)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.015862005413236926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ejfO8FAEJ5w"
      },
      "source": [
        "## Create submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ros-VlYTSWuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d01bcd-6ef7-4e97-c3ce-20f9886e85ec"
      },
      "source": [
        "%%time\n",
        "\n",
        "submission = nn_trainer.run_multiple_seeds(seeds=[42], num_folds=7, run_type=\"submission\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 12.5 s, sys: 534 ms, total: 13.1 s\n",
            "Wall time: 12.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQVqfSYemmlG",
        "outputId": "7d993a7d-b8c1-48da-e2db-0d67b02ad5a7"
      },
      "source": [
        "submission.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3982, 207)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X-Y6A-4qgyF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "5c1b612b-3e35-4298-cc77-0f15c9cadc03"
      },
      "source": [
        "submission"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_0004d9e33</td>\n",
              "      <td>0.001465</td>\n",
              "      <td>0.001591</td>\n",
              "      <td>0.002146</td>\n",
              "      <td>0.016962</td>\n",
              "      <td>0.024648</td>\n",
              "      <td>0.005407</td>\n",
              "      <td>0.004150</td>\n",
              "      <td>0.004173</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.011175</td>\n",
              "      <td>0.022979</td>\n",
              "      <td>0.001304</td>\n",
              "      <td>0.000662</td>\n",
              "      <td>0.000709</td>\n",
              "      <td>0.001376</td>\n",
              "      <td>0.001372</td>\n",
              "      <td>0.003506</td>\n",
              "      <td>0.005864</td>\n",
              "      <td>0.004618</td>\n",
              "      <td>0.002225</td>\n",
              "      <td>0.003078</td>\n",
              "      <td>0.004402</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.002298</td>\n",
              "      <td>0.000997</td>\n",
              "      <td>0.000934</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.001345</td>\n",
              "      <td>0.004887</td>\n",
              "      <td>0.002707</td>\n",
              "      <td>0.002056</td>\n",
              "      <td>0.002920</td>\n",
              "      <td>0.003273</td>\n",
              "      <td>0.000472</td>\n",
              "      <td>0.000554</td>\n",
              "      <td>0.000585</td>\n",
              "      <td>0.002517</td>\n",
              "      <td>0.000558</td>\n",
              "      <td>0.000931</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003583</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.004556</td>\n",
              "      <td>0.001328</td>\n",
              "      <td>0.000827</td>\n",
              "      <td>0.002951</td>\n",
              "      <td>0.001039</td>\n",
              "      <td>0.001173</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>0.001682</td>\n",
              "      <td>0.014755</td>\n",
              "      <td>0.017149</td>\n",
              "      <td>0.002927</td>\n",
              "      <td>0.002875</td>\n",
              "      <td>0.001892</td>\n",
              "      <td>0.001782</td>\n",
              "      <td>0.019545</td>\n",
              "      <td>0.002410</td>\n",
              "      <td>0.000970</td>\n",
              "      <td>0.001024</td>\n",
              "      <td>0.000764</td>\n",
              "      <td>0.002668</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.001401</td>\n",
              "      <td>0.001884</td>\n",
              "      <td>0.002368</td>\n",
              "      <td>0.001024</td>\n",
              "      <td>0.002525</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.001379</td>\n",
              "      <td>0.001074</td>\n",
              "      <td>0.001207</td>\n",
              "      <td>0.004416</td>\n",
              "      <td>0.002474</td>\n",
              "      <td>0.001538</td>\n",
              "      <td>0.000849</td>\n",
              "      <td>0.001475</td>\n",
              "      <td>0.002229</td>\n",
              "      <td>0.002205</td>\n",
              "      <td>0.001841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_001897cda</td>\n",
              "      <td>0.001032</td>\n",
              "      <td>0.001075</td>\n",
              "      <td>0.001089</td>\n",
              "      <td>0.002363</td>\n",
              "      <td>0.001492</td>\n",
              "      <td>0.002366</td>\n",
              "      <td>0.004590</td>\n",
              "      <td>0.019142</td>\n",
              "      <td>0.029156</td>\n",
              "      <td>0.033403</td>\n",
              "      <td>0.008973</td>\n",
              "      <td>0.002155</td>\n",
              "      <td>0.000697</td>\n",
              "      <td>0.009224</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.000830</td>\n",
              "      <td>0.001175</td>\n",
              "      <td>0.001868</td>\n",
              "      <td>0.001725</td>\n",
              "      <td>0.004787</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.001148</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.001578</td>\n",
              "      <td>0.001548</td>\n",
              "      <td>0.003576</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>0.000727</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.002346</td>\n",
              "      <td>0.001188</td>\n",
              "      <td>0.001236</td>\n",
              "      <td>0.001265</td>\n",
              "      <td>0.001390</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.008582</td>\n",
              "      <td>0.008053</td>\n",
              "      <td>0.003697</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001615</td>\n",
              "      <td>0.001353</td>\n",
              "      <td>0.000924</td>\n",
              "      <td>0.000738</td>\n",
              "      <td>0.001689</td>\n",
              "      <td>0.001216</td>\n",
              "      <td>0.000656</td>\n",
              "      <td>0.031223</td>\n",
              "      <td>0.000660</td>\n",
              "      <td>0.003287</td>\n",
              "      <td>0.018633</td>\n",
              "      <td>0.005614</td>\n",
              "      <td>0.001596</td>\n",
              "      <td>0.001166</td>\n",
              "      <td>0.002910</td>\n",
              "      <td>0.002150</td>\n",
              "      <td>0.008028</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.013970</td>\n",
              "      <td>0.000560</td>\n",
              "      <td>0.009579</td>\n",
              "      <td>0.006060</td>\n",
              "      <td>0.001661</td>\n",
              "      <td>0.000774</td>\n",
              "      <td>0.000681</td>\n",
              "      <td>0.001312</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>0.003488</td>\n",
              "      <td>0.002088</td>\n",
              "      <td>0.001983</td>\n",
              "      <td>0.000533</td>\n",
              "      <td>0.001404</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.000610</td>\n",
              "      <td>0.018066</td>\n",
              "      <td>0.000670</td>\n",
              "      <td>0.008099</td>\n",
              "      <td>0.002305</td>\n",
              "      <td>0.001275</td>\n",
              "      <td>0.004330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_002429b5b</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_00276f245</td>\n",
              "      <td>0.001279</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.001622</td>\n",
              "      <td>0.011054</td>\n",
              "      <td>0.020669</td>\n",
              "      <td>0.004152</td>\n",
              "      <td>0.003879</td>\n",
              "      <td>0.006234</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>0.011068</td>\n",
              "      <td>0.027586</td>\n",
              "      <td>0.002134</td>\n",
              "      <td>0.000666</td>\n",
              "      <td>0.001530</td>\n",
              "      <td>0.001378</td>\n",
              "      <td>0.001386</td>\n",
              "      <td>0.002987</td>\n",
              "      <td>0.006298</td>\n",
              "      <td>0.005127</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>0.001942</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.002733</td>\n",
              "      <td>0.001232</td>\n",
              "      <td>0.001153</td>\n",
              "      <td>0.001952</td>\n",
              "      <td>0.002452</td>\n",
              "      <td>0.006023</td>\n",
              "      <td>0.002092</td>\n",
              "      <td>0.002118</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>0.003061</td>\n",
              "      <td>0.001499</td>\n",
              "      <td>0.000688</td>\n",
              "      <td>0.000661</td>\n",
              "      <td>0.003551</td>\n",
              "      <td>0.000796</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>0.001168</td>\n",
              "      <td>0.002913</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.001701</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.000818</td>\n",
              "      <td>0.000701</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.000994</td>\n",
              "      <td>0.016972</td>\n",
              "      <td>0.057865</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>0.002339</td>\n",
              "      <td>0.004424</td>\n",
              "      <td>0.002539</td>\n",
              "      <td>0.011522</td>\n",
              "      <td>0.001833</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.000761</td>\n",
              "      <td>0.000599</td>\n",
              "      <td>0.006065</td>\n",
              "      <td>0.000393</td>\n",
              "      <td>0.002064</td>\n",
              "      <td>0.001483</td>\n",
              "      <td>0.002699</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.002404</td>\n",
              "      <td>0.000791</td>\n",
              "      <td>0.001641</td>\n",
              "      <td>0.000717</td>\n",
              "      <td>0.001249</td>\n",
              "      <td>0.002628</td>\n",
              "      <td>0.003433</td>\n",
              "      <td>0.003375</td>\n",
              "      <td>0.000715</td>\n",
              "      <td>0.001563</td>\n",
              "      <td>0.002037</td>\n",
              "      <td>0.003058</td>\n",
              "      <td>0.002001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_0027f1083</td>\n",
              "      <td>0.001972</td>\n",
              "      <td>0.001777</td>\n",
              "      <td>0.001723</td>\n",
              "      <td>0.015567</td>\n",
              "      <td>0.023992</td>\n",
              "      <td>0.005800</td>\n",
              "      <td>0.004231</td>\n",
              "      <td>0.003051</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>0.014584</td>\n",
              "      <td>0.025292</td>\n",
              "      <td>0.001420</td>\n",
              "      <td>0.000513</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.001310</td>\n",
              "      <td>0.003933</td>\n",
              "      <td>0.005511</td>\n",
              "      <td>0.004194</td>\n",
              "      <td>0.001972</td>\n",
              "      <td>0.002998</td>\n",
              "      <td>0.004643</td>\n",
              "      <td>0.000922</td>\n",
              "      <td>0.002218</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.000916</td>\n",
              "      <td>0.000979</td>\n",
              "      <td>0.001126</td>\n",
              "      <td>0.004959</td>\n",
              "      <td>0.003090</td>\n",
              "      <td>0.001832</td>\n",
              "      <td>0.002797</td>\n",
              "      <td>0.003992</td>\n",
              "      <td>0.000432</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.002239</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003611</td>\n",
              "      <td>0.001143</td>\n",
              "      <td>0.005665</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.000765</td>\n",
              "      <td>0.002576</td>\n",
              "      <td>0.000954</td>\n",
              "      <td>0.001291</td>\n",
              "      <td>0.001650</td>\n",
              "      <td>0.001817</td>\n",
              "      <td>0.011102</td>\n",
              "      <td>0.017334</td>\n",
              "      <td>0.002940</td>\n",
              "      <td>0.002656</td>\n",
              "      <td>0.001709</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>0.019241</td>\n",
              "      <td>0.001934</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000983</td>\n",
              "      <td>0.000770</td>\n",
              "      <td>0.003159</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.002122</td>\n",
              "      <td>0.002180</td>\n",
              "      <td>0.000952</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.002027</td>\n",
              "      <td>0.001322</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>0.000835</td>\n",
              "      <td>0.003511</td>\n",
              "      <td>0.002788</td>\n",
              "      <td>0.001494</td>\n",
              "      <td>0.000909</td>\n",
              "      <td>0.001166</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.001378</td>\n",
              "      <td>0.001986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3977</th>\n",
              "      <td>id_ff7004b87</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.001694</td>\n",
              "      <td>0.004154</td>\n",
              "      <td>0.003072</td>\n",
              "      <td>0.001340</td>\n",
              "      <td>0.001898</td>\n",
              "      <td>0.004565</td>\n",
              "      <td>0.000585</td>\n",
              "      <td>0.002180</td>\n",
              "      <td>0.004996</td>\n",
              "      <td>0.002310</td>\n",
              "      <td>0.002377</td>\n",
              "      <td>0.024155</td>\n",
              "      <td>0.001055</td>\n",
              "      <td>0.001187</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>0.003131</td>\n",
              "      <td>0.002304</td>\n",
              "      <td>0.002256</td>\n",
              "      <td>0.001084</td>\n",
              "      <td>0.001729</td>\n",
              "      <td>0.001596</td>\n",
              "      <td>0.002352</td>\n",
              "      <td>0.000839</td>\n",
              "      <td>0.004236</td>\n",
              "      <td>0.001546</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.002855</td>\n",
              "      <td>0.000693</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.003056</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>0.000995</td>\n",
              "      <td>0.016796</td>\n",
              "      <td>0.010426</td>\n",
              "      <td>0.050614</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007396</td>\n",
              "      <td>0.002331</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.000673</td>\n",
              "      <td>0.001020</td>\n",
              "      <td>0.001278</td>\n",
              "      <td>0.000924</td>\n",
              "      <td>0.002575</td>\n",
              "      <td>0.000576</td>\n",
              "      <td>0.000724</td>\n",
              "      <td>0.013570</td>\n",
              "      <td>0.004780</td>\n",
              "      <td>0.001076</td>\n",
              "      <td>0.000908</td>\n",
              "      <td>0.002376</td>\n",
              "      <td>0.001269</td>\n",
              "      <td>0.006700</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.031804</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.001822</td>\n",
              "      <td>0.001765</td>\n",
              "      <td>0.001998</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.000482</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.000649</td>\n",
              "      <td>0.001451</td>\n",
              "      <td>0.000442</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.008484</td>\n",
              "      <td>0.002844</td>\n",
              "      <td>0.104799</td>\n",
              "      <td>0.010246</td>\n",
              "      <td>0.001053</td>\n",
              "      <td>0.004915</td>\n",
              "      <td>0.001437</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.000697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3978</th>\n",
              "      <td>id_ff925dd0d</td>\n",
              "      <td>0.001983</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>0.001751</td>\n",
              "      <td>0.010068</td>\n",
              "      <td>0.018437</td>\n",
              "      <td>0.005669</td>\n",
              "      <td>0.005648</td>\n",
              "      <td>0.004829</td>\n",
              "      <td>0.001063</td>\n",
              "      <td>0.019823</td>\n",
              "      <td>0.026821</td>\n",
              "      <td>0.002307</td>\n",
              "      <td>0.000578</td>\n",
              "      <td>0.000722</td>\n",
              "      <td>0.000958</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.003150</td>\n",
              "      <td>0.004876</td>\n",
              "      <td>0.003837</td>\n",
              "      <td>0.003002</td>\n",
              "      <td>0.002410</td>\n",
              "      <td>0.003864</td>\n",
              "      <td>0.000921</td>\n",
              "      <td>0.002331</td>\n",
              "      <td>0.001503</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.001092</td>\n",
              "      <td>0.001240</td>\n",
              "      <td>0.004037</td>\n",
              "      <td>0.003680</td>\n",
              "      <td>0.001927</td>\n",
              "      <td>0.002433</td>\n",
              "      <td>0.003882</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>0.000619</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.003636</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.001034</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>0.001454</td>\n",
              "      <td>0.005119</td>\n",
              "      <td>0.001957</td>\n",
              "      <td>0.001001</td>\n",
              "      <td>0.001806</td>\n",
              "      <td>0.000935</td>\n",
              "      <td>0.001970</td>\n",
              "      <td>0.001403</td>\n",
              "      <td>0.002030</td>\n",
              "      <td>0.012207</td>\n",
              "      <td>0.019847</td>\n",
              "      <td>0.003013</td>\n",
              "      <td>0.002979</td>\n",
              "      <td>0.002040</td>\n",
              "      <td>0.002007</td>\n",
              "      <td>0.018906</td>\n",
              "      <td>0.001417</td>\n",
              "      <td>0.001808</td>\n",
              "      <td>0.000877</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.004253</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>0.001498</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>0.000952</td>\n",
              "      <td>0.003072</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.001557</td>\n",
              "      <td>0.000818</td>\n",
              "      <td>0.001054</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.002905</td>\n",
              "      <td>0.002288</td>\n",
              "      <td>0.000854</td>\n",
              "      <td>0.002638</td>\n",
              "      <td>0.001883</td>\n",
              "      <td>0.001739</td>\n",
              "      <td>0.002383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3979</th>\n",
              "      <td>id_ffb710450</td>\n",
              "      <td>0.002299</td>\n",
              "      <td>0.001954</td>\n",
              "      <td>0.001523</td>\n",
              "      <td>0.012810</td>\n",
              "      <td>0.032309</td>\n",
              "      <td>0.006448</td>\n",
              "      <td>0.003840</td>\n",
              "      <td>0.003467</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>0.015424</td>\n",
              "      <td>0.029545</td>\n",
              "      <td>0.001103</td>\n",
              "      <td>0.000416</td>\n",
              "      <td>0.000723</td>\n",
              "      <td>0.001283</td>\n",
              "      <td>0.001143</td>\n",
              "      <td>0.005181</td>\n",
              "      <td>0.007297</td>\n",
              "      <td>0.006554</td>\n",
              "      <td>0.001833</td>\n",
              "      <td>0.003352</td>\n",
              "      <td>0.004753</td>\n",
              "      <td>0.000902</td>\n",
              "      <td>0.001703</td>\n",
              "      <td>0.001186</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>0.000870</td>\n",
              "      <td>0.001128</td>\n",
              "      <td>0.005308</td>\n",
              "      <td>0.003112</td>\n",
              "      <td>0.002125</td>\n",
              "      <td>0.002419</td>\n",
              "      <td>0.003807</td>\n",
              "      <td>0.000455</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.000393</td>\n",
              "      <td>0.002034</td>\n",
              "      <td>0.000417</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003574</td>\n",
              "      <td>0.001222</td>\n",
              "      <td>0.006076</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.003689</td>\n",
              "      <td>0.000859</td>\n",
              "      <td>0.001396</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.012139</td>\n",
              "      <td>0.024091</td>\n",
              "      <td>0.002923</td>\n",
              "      <td>0.002752</td>\n",
              "      <td>0.002145</td>\n",
              "      <td>0.001020</td>\n",
              "      <td>0.020692</td>\n",
              "      <td>0.001691</td>\n",
              "      <td>0.000931</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.003134</td>\n",
              "      <td>0.000602</td>\n",
              "      <td>0.002036</td>\n",
              "      <td>0.002072</td>\n",
              "      <td>0.002449</td>\n",
              "      <td>0.000832</td>\n",
              "      <td>0.002043</td>\n",
              "      <td>0.001970</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.003578</td>\n",
              "      <td>0.001966</td>\n",
              "      <td>0.001463</td>\n",
              "      <td>0.000933</td>\n",
              "      <td>0.001446</td>\n",
              "      <td>0.001895</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>0.001655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3980</th>\n",
              "      <td>id_ffbb869f2</td>\n",
              "      <td>0.002392</td>\n",
              "      <td>0.001559</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.013928</td>\n",
              "      <td>0.027880</td>\n",
              "      <td>0.006124</td>\n",
              "      <td>0.004950</td>\n",
              "      <td>0.003572</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.023560</td>\n",
              "      <td>0.035277</td>\n",
              "      <td>0.001069</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000776</td>\n",
              "      <td>0.001207</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.006346</td>\n",
              "      <td>0.005557</td>\n",
              "      <td>0.002115</td>\n",
              "      <td>0.002574</td>\n",
              "      <td>0.003989</td>\n",
              "      <td>0.000817</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.001239</td>\n",
              "      <td>0.000850</td>\n",
              "      <td>0.000991</td>\n",
              "      <td>0.000992</td>\n",
              "      <td>0.005623</td>\n",
              "      <td>0.003303</td>\n",
              "      <td>0.001925</td>\n",
              "      <td>0.001965</td>\n",
              "      <td>0.004049</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>0.000415</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003402</td>\n",
              "      <td>0.001172</td>\n",
              "      <td>0.006536</td>\n",
              "      <td>0.000565</td>\n",
              "      <td>0.000848</td>\n",
              "      <td>0.001837</td>\n",
              "      <td>0.000716</td>\n",
              "      <td>0.001408</td>\n",
              "      <td>0.001303</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.013308</td>\n",
              "      <td>0.022459</td>\n",
              "      <td>0.003388</td>\n",
              "      <td>0.003453</td>\n",
              "      <td>0.002276</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.020718</td>\n",
              "      <td>0.001745</td>\n",
              "      <td>0.000920</td>\n",
              "      <td>0.000790</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.004163</td>\n",
              "      <td>0.000523</td>\n",
              "      <td>0.001998</td>\n",
              "      <td>0.001750</td>\n",
              "      <td>0.002587</td>\n",
              "      <td>0.000854</td>\n",
              "      <td>0.002446</td>\n",
              "      <td>0.001411</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000768</td>\n",
              "      <td>0.000536</td>\n",
              "      <td>0.002684</td>\n",
              "      <td>0.001379</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.001223</td>\n",
              "      <td>0.001958</td>\n",
              "      <td>0.001310</td>\n",
              "      <td>0.002293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3981</th>\n",
              "      <td>id_ffd5800b6</td>\n",
              "      <td>0.001179</td>\n",
              "      <td>0.001717</td>\n",
              "      <td>0.002357</td>\n",
              "      <td>0.018095</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.004940</td>\n",
              "      <td>0.004250</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.000480</td>\n",
              "      <td>0.008012</td>\n",
              "      <td>0.019094</td>\n",
              "      <td>0.001422</td>\n",
              "      <td>0.000871</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.001355</td>\n",
              "      <td>0.001572</td>\n",
              "      <td>0.003861</td>\n",
              "      <td>0.006081</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.002020</td>\n",
              "      <td>0.003636</td>\n",
              "      <td>0.004320</td>\n",
              "      <td>0.000950</td>\n",
              "      <td>0.002817</td>\n",
              "      <td>0.000927</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.001212</td>\n",
              "      <td>0.001457</td>\n",
              "      <td>0.005792</td>\n",
              "      <td>0.002374</td>\n",
              "      <td>0.001666</td>\n",
              "      <td>0.004105</td>\n",
              "      <td>0.003513</td>\n",
              "      <td>0.000417</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>0.000576</td>\n",
              "      <td>0.003191</td>\n",
              "      <td>0.000662</td>\n",
              "      <td>0.000844</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004679</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.003629</td>\n",
              "      <td>0.001443</td>\n",
              "      <td>0.000736</td>\n",
              "      <td>0.003901</td>\n",
              "      <td>0.001187</td>\n",
              "      <td>0.001093</td>\n",
              "      <td>0.001491</td>\n",
              "      <td>0.001586</td>\n",
              "      <td>0.013619</td>\n",
              "      <td>0.014636</td>\n",
              "      <td>0.002947</td>\n",
              "      <td>0.001944</td>\n",
              "      <td>0.001856</td>\n",
              "      <td>0.001338</td>\n",
              "      <td>0.015459</td>\n",
              "      <td>0.001985</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.001020</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>0.002659</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.001314</td>\n",
              "      <td>0.002178</td>\n",
              "      <td>0.001861</td>\n",
              "      <td>0.000991</td>\n",
              "      <td>0.002308</td>\n",
              "      <td>0.001733</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.001017</td>\n",
              "      <td>0.001793</td>\n",
              "      <td>0.004540</td>\n",
              "      <td>0.005617</td>\n",
              "      <td>0.001363</td>\n",
              "      <td>0.001055</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.001998</td>\n",
              "      <td>0.001496</td>\n",
              "      <td>0.001551</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3982 rows × 207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            sig_id  ...  wnt_inhibitor\n",
              "0     id_0004d9e33  ...       0.001841\n",
              "1     id_001897cda  ...       0.004330\n",
              "2     id_002429b5b  ...       0.000000\n",
              "3     id_00276f245  ...       0.002001\n",
              "4     id_0027f1083  ...       0.001986\n",
              "...            ...  ...            ...\n",
              "3977  id_ff7004b87  ...       0.000697\n",
              "3978  id_ff925dd0d  ...       0.002383\n",
              "3979  id_ffb710450  ...       0.001655\n",
              "3980  id_ffbb869f2  ...       0.002293\n",
              "3981  id_ffd5800b6  ...       0.001551\n",
              "\n",
              "[3982 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik58AWlKzODM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}